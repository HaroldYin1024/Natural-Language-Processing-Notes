{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An alternative approach\n",
    "Can we define words by the company they keep?  \n",
    "\"If A and B have almost the identical environments we can say that they are synonyms\" (Zelig Harris, 1954)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representations\n",
    "One-hot encodings are long and sparse  \n",
    "Alternative: **dense vectors**  \n",
    "short (length 50-1000) + dense (most elements are non-zero)\n",
    "### Benefits\n",
    "1. Easier to use in ML  \n",
    "2. Offer better generalization capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train your own embeddings\n",
    "Instead of counting terms, we train a classifier on a prediction task: 'does A occur near B'?  \n",
    "The learned classifier weights become our embeddings   \n",
    "We can create our own word embeddings using gensim (an open-source library for unsupervised topic modeling and NLP)  \n",
    "and train it on the Brown corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up libraries & data\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = gensim.models.Word2Vec(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f89c16ccc10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy, for later re-use\n",
    "model.save('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load models on demand\n",
    "brown_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words?\n",
    "len(brown_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many dimensions?\n",
    "len(brown_model.wv['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1123121 ,  0.25699806,  0.20049234,  0.11121298, -0.06425545,\n",
       "       -0.33215448,  0.203627  ,  0.347207  , -0.30725956, -0.28279257,\n",
       "        0.16802801, -0.22751583,  0.19008662,  0.1674023 ,  0.24391298,\n",
       "       -0.16353872,  0.26953107, -0.14396475, -0.5229727 , -0.5176938 ,\n",
       "        0.28552625, -0.11481584,  0.48428005,  0.07612962, -0.05926904,\n",
       "       -0.13284211, -0.23031797,  0.01688239, -0.2100438 ,  0.23463464,\n",
       "        0.2175537 , -0.06709159,  0.28098744, -0.37103644, -0.1776211 ,\n",
       "        0.05884128, -0.18912491, -0.05309489, -0.34382743, -0.05950236,\n",
       "        0.0128465 , -0.26842707,  0.18020464,  0.11521347,  0.20342   ,\n",
       "       -0.00789078, -0.01889006, -0.02543117,  0.09976728,  0.31008193,\n",
       "        0.01597053, -0.28622678, -0.25172043, -0.19010486, -0.10741533,\n",
       "       -0.22977918,  0.17676017,  0.052156  , -0.06199336, -0.07945454,\n",
       "        0.02969945,  0.20052046, -0.03786999, -0.16787542, -0.18871498,\n",
       "        0.45194843,  0.02368015,  0.2838704 , -0.26320067,  0.36976248,\n",
       "        0.20501325,  0.14616658,  0.21652249, -0.00202802,  0.33394107,\n",
       "        0.09029943,  0.10729884,  0.04793023,  0.02991804, -0.12938808,\n",
       "       -0.11856803,  0.01157363, -0.21765538,  0.07352746, -0.1856942 ,\n",
       "       -0.01782224,  0.1898568 ,  0.01796352,  0.16547082,  0.0751299 ,\n",
       "        0.42599267, -0.15173976, -0.11973429,  0.11236047,  0.32337347,\n",
       "        0.12011313,  0.16828266, -0.3635182 , -0.15229046,  0.05143666],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "brown_model.wv['university']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8153878"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate similarity between terms\n",
    "brown_model.wv.similarity('university','school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9542639255523682),\n",
       " ('profession', 0.9534994959831238),\n",
       " ('neighborhood', 0.9527466297149658),\n",
       " ('congregation', 0.9513265490531921),\n",
       " ('selection', 0.9480083584785461)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar terms\n",
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marble', 0.9657989740371704),\n",
       " ('pension', 0.9648841619491577),\n",
       " ('frankfurters', 0.9640069007873535),\n",
       " ('herd', 0.9626672267913818),\n",
       " ('towel', 0.9624289870262146)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('lemon', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nation', 0.9279634952545166),\n",
       " ('policy', 0.9250583648681641),\n",
       " ('Christian', 0.9233180284500122),\n",
       " ('power', 0.9231869578361511),\n",
       " ('education', 0.9204767346382141)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('government', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use pre-trained embeddings\n",
    "We can load pre-built embeddings, e.g. a sample from a model trained on 100 billion words from the Google News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('word2vec_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-build model\n",
    "from nltk.data import find\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "news_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43981"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many terms?\n",
    "len(news_model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many dimensions?\n",
    "len(news_model['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are they any better?\n",
    "news_model.most_similar(positive=['university'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lemons', 0.646256148815155),\n",
       " ('apricot', 0.6199417114257812),\n",
       " ('avocado', 0.5922889113426208),\n",
       " ('fennel', 0.5873182415962219),\n",
       " ('coriander', 0.5828487277030945)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['lemon'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Government', 0.7132059335708618),\n",
       " ('governments', 0.6521531939506531),\n",
       " ('administration', 0.5462369322776794),\n",
       " ('legislature', 0.5307289361953735),\n",
       " ('parliament', 0.5268454551696777)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['government'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform vector algebra\n",
    "We can use embeddings to perform verbal reasoning, e.g. A is to B as C is to...  \n",
    "e.g. 'man is to king as woman is to...'  \n",
    "vec(“king”) - vec(“man”) + vec(“woman”) =~ vec(“queen”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['woman','king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902430415153503),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236843228340149)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['king', 'woman'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091734886169),\n",
       " ('Belgium', 0.6197876930236816),\n",
       " ('Spain', 0.5664774179458618),\n",
       " ('Italy', 0.5654898881912231),\n",
       " ('Switzerland', 0.560969352722168)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 0.6774996519088745),\n",
       " ('was', 0.5710028409957886),\n",
       " ('remains', 0.47552669048309326),\n",
       " ('been', 0.4538103938102722),\n",
       " ('being', 0.4456518888473511)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic patterns (verbs)\n",
    "news_model.most_similar(positive=['has','be'], negative=['have'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shortest', 0.5145131945610046),\n",
       " ('steepest', 0.42448338866233826),\n",
       " ('first', 0.40251171588897705),\n",
       " ('flattest', 0.40171927213668823),\n",
       " ('consecutive', 0.39518705010414124)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic patterns (adjectives)\n",
    "news_model.most_similar(positive=['longest','short'], negative=['long'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('purple', 0.5252774953842163),\n",
       " ('tulips', 0.4938238859176636),\n",
       " ('brown', 0.4907749593257904),\n",
       " ('pink', 0.4860530197620392),\n",
       " ('maroon', 0.48056456446647644)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# more encyclopaedic knowledge\n",
    "news_model.most_similar(positive=['blue','tulip'], negative=['sky'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her', 0.804938554763794),\n",
       " ('herself', 0.6881042718887329),\n",
       " ('me', 0.5886672139167786),\n",
       " ('She', 0.5803765058517456),\n",
       " ('woman', 0.5470799207687378)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syntactic knowledge (pronouns)\n",
    "news_model.most_similar(positive=['him','she'], negative=['he'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('short', 0.4077243208885193),\n",
       " ('longer', 0.3670078217983246),\n",
       " ('lengthy', 0.36229580640792847),\n",
       " ('Long', 0.36003556847572327),\n",
       " ('continuous', 0.34982720017433167)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical know\n",
    "news_model.most_similar(positive=['light','long'], negative=['dark'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the odd one out\n",
    "news_model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz questions + homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('have', 0.7298133969306946),\n",
       " ('Had', 0.5585500597953796),\n",
       " ('subsequently', 0.54400634765625),\n",
       " ('had', 0.5378279685974121),\n",
       " ('recently', 0.5339969992637634)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar(positive=['be','has'], negative=['is'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('membership', 0.9542639255523682),\n",
       " ('profession', 0.9534994959831238),\n",
       " ('neighborhood', 0.9527466297149658),\n",
       " ('congregation', 0.9513265490531921),\n",
       " ('selection', 0.9480083584785461)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013),\n",
       " ('campus', 0.6434987783432007),\n",
       " ('college', 0.638526976108551)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar('university', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('university', 0.9233660101890564),\n",
       " ('treatment', 0.9230210781097412),\n",
       " ('selection', 0.9215536713600159),\n",
       " ('persons', 0.9178534150123596),\n",
       " ('generations', 0.9160250425338745)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_model.wv.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('colleges', 0.6560819149017334),\n",
       " ('university', 0.6385270357131958),\n",
       " ('school', 0.6081898808479309),\n",
       " ('collegiate', 0.6081600189208984),\n",
       " ('undergraduate', 0.5866836905479431)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.most_similar('college', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054503173"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university','turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5080746"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13740103"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'factory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13598306"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'supermarket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054503173"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_model.similarity('university', 'turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
